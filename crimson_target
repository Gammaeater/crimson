#!/bin/bash
pid=$$
# Delete below line if you install Crimson from source:
PATH="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/go/bin:/root/go/bin"
#
### CREATED BY KARMAZ
#
#
#
### FUNCTIONS:
#
# 1. FULL RANGE PORT SCAN && NSE ON OPENED PORTS
# 2. VULN SCANS
# 3. SPIDER THE DOMAIN
# 4. DIRECTORY BRUTEFORCE
# 5. GATHER SOURCE CODE OF SCRAPED / BRUTEFORCED URLS
# 6. EXTRACT NEW PATHS, API KEYS, ENDPOINTS FROM GATHERED SOURCE CODE
# 7. MERGE PATHS WITH DOMAIN AND PROBE FOR NEW ENDPOINTS
# 8. SEND ALL FINDINGS WITHOUT 400/404 STATUS CODE TO BURP PROXY
# 9. PREPARE params.txt FOR EXPLOIT MODULE
# 10. PREPARE dirs.txt FOR EXPLOIT MODULE
# 11. CHECK WAF
# 12. CHECK POTENTIAL BACKUP FILES
# 13. CHECK CMS
# 14. TEST HOP-BY-HOP DELETION
#
### LISTS:
#
# 1) recon.txt          - FILE WITH RECON OUTPUT
# 2) urls.txt           - FILE WITH GATHERED URLS
# 3) status_params.txt  - STATUS CODES OF urls.txt
# 4) ffuf.txt           - DIR BRUTEFORCING OUTPUT
# 5) status_dir.txt     - STATUS CODE OF ffuf.txt
# 9) exp/params.txt     - FILE PREPARED FOR crimson_exploit WITH PARAMS
# 10) exp/dirs.txt      - FILE PREPARED FOR crimson_exploit WITH DIRECTORIES
# 11) backups.txt       - POTENTIALLY BACKUP FILES 
# 12) arjun.txt         - FILE WITH BRUTEFORCED PARAMETERS
# 13) nmap.txt          - FILE WITH TCP/UDP PORT SCANNING OUTPUT
# 15) exp/nmap.gnmap    - FILE WITH TCP/UDP PORT SCANNING OUTPUT IN GREPABLE FORMAT 
#
### WORKFLOW
#
# 0. Start Burp
#   - Create new project - www.example.tld
#   - Turn off interception
#   - Make active scan for proxied urls only in scope
# 1. Start the script
# 2. Check the output listed above (LISTS)
# 3. Manually browse the application, click on all functionalities
# 4. Copy whole target scope from Burp after manually browsing the target
# 5. Paste it to exp/all.txt and run crimson_exploit
#
###
echo -e "\033[0;31m
 ██████╗██████╗ ██╗███╗   ███╗███████╗ ██████╗ ███╗   ██╗     ████████╗ █████╗ ██████╗  ██████╗ ███████╗████████╗
██╔════╝██╔══██╗██║████╗ ████║██╔════╝██╔═══██╗████╗  ██║     ╚══██╔══╝██╔══██╗██╔══██╗██╔════╝ ██╔════╝╚══██╔══╝
██║     ██████╔╝██║██╔████╔██║███████╗██║   ██║██╔██╗ ██║        ██║   ███████║██████╔╝██║  ███╗█████╗     ██║   
██║     ██╔══██╗██║██║╚██╔╝██║╚════██║██║   ██║██║╚██╗██║        ██║   ██╔══██║██╔══██╗██║   ██║██╔══╝     ██║   
╚██████╗██║  ██║██║██║ ╚═╝ ██║███████║╚██████╔╝██║ ╚████║███████╗██║   ██║  ██║██║  ██║╚██████╔╝███████╗   ██║   
 ╚═════╝╚═╝  ╚═╝╚═╝╚═╝     ╚═╝╚══════╝ ╚═════╝ ╚═╝  ╚═══╝╚══════╝╚═╝   ╚═╝  ╚═╝╚═╝  ╚═╝ ╚═════╝ ╚══════╝   ╚═╝   
                                                                                                                 
\033[0m"

rust_on=0
fully_automated=0
proxy_on=0
udp_on=0
while getopts "c:D:payu" OPTION; do
    case $OPTION in
    c)
        cookie=$OPTARG
        ;;
    D)
        domain=$OPTARG
        ;;
    p)
        rust_on=1
        ;;
    u)
        udp_on=1
        ;;
    a)
        fully_automated=1
        ;;
    y)
        proxy_on=1
        ;;
    *)
        echo "Incorrect options provided"
        exit 1
        ;;
    esac
done

if [ -z "$domain" ]
then
    echo "./crimson_target -D \"example.domain.com\" 
                    
                    # Optional flags are shown below:
                 -c \"Cookie: auth1=123;\"
                 -p # TCP (1-65535)   ports scanning
                 -u # UDP (nmap default) ports scanning
                 -a # Without this flag, you have to manually check for false-positives after bruteforcing
                 -y # Proxy urls.txt and ffuf.txt to Burp (127.0.0.1:8080)"
    exit  1
else
    ### PREPARE DIRECTORIES AND VARIABLES
    echo -e "\033[0;31m [+]\033[0m PREPARING DIRECTORIES AND VARIABLES"
    export domain
    DOMAIN=$(tldextract "$domain" | cut -s -d " " -f 2-3 | sed "s/\ /\./")
    export DOMAIN
    TARGET=$(dig +short "$domain" | tr "\n" ",")
    export TARGET
    mkdir "$HOME"/bounty/"$DOMAIN"/"$domain"/temp -p
    mkdir "$HOME"/bounty/"$DOMAIN"/"$domain"/exp
    mkdir "$HOME"/bounty/"$DOMAIN"/"$domain"/all_source_code
    cd "$HOME"/bounty/"$DOMAIN"/"$domain" || exit   
    
    if [ -z "$cookie" ]
    then
        export cookie="Cookie: a=1;";
    else
        export cookie=$cookie;
    fi
    
    if wget --spider --no-check-certificate https://"$domain" 2>/dev/null; then
        https_on=1
    else
        https_on=0
    fi

    echo "----------------------------------------------------------------"
    echo "SCANNING START: $(date +'[%m-%d %H:%M:%S]')" >> recon.txt    
    echo "----------------------------------------------------------------"

    ### --- PORT SCANNING SECTION --- ### -p
    if [ $rust_on == 1 ]
    then
        ### RESOLVE IP AND SCAN OPENED PORTS > recon.txt
        echo "[TCP SCANNING] ---------------------------------------------------------" | tee -a recon.txt
        echo TARGET: "$TARGET" | tee -a recon.txt
        rustscan -a "$TARGET" --ulimit 5000 -- -n -A -Pn --append-output -oG exp/nmap.gnmap -oN nmap.txt  # --scan-order "Random"
        cat nmap.txt >> recon.txt
        rm nmap.txt
    fi
    
    if [ $udp_on == 1 ]
    then
        echo "[UDP SCANNING] ---------------------------------------------------------" | tee -a recon.txt
        echo TARGET: "$TARGET" | tee -a recon.txt
        nmap -sU "$domain" -A -Pn --append-output -oG exp/udpnmap.gnmap -oN udpnmap.txt || sudo nmap -sU "$domain" -A -Pn --append-output -oG exp/udpnmap.gnmap -oN udpnmap.txt
        cat udpnmap.txt >> recon.txt
    fi

    ### GET THE CONTENT OF SITEMAP IF EXISTS >> recon.txt2
    echo "[SITEMAP 80] ----------------------------------------------------------" | tee -a recon.txt
    "$HOME"/tools/sitemap-urls/sitemap-urls.sh http://"$domain"/sitemap.xml >> recon.txt
    echo >> recon.txt
    echo "[SITEMAP 443] ---------------------------------------------------------" | tee -a recon.txt
    "$HOME"/tools/sitemap-urls/sitemap-urls.sh https://"$domain"/sitemap.xml >> recon.txt
    echo >> recon.txt

    ### GET THE CONTENT OF ROBOTS IF EXISTS >> recon.txt
    echo "[ROBOTS 80] -----------------------------------------------------------" | tee -a recon.txt
    curl -O http://"$domain"/robots.txt > /dev/null 2>&1
    cat robots.txt >> recon.txt
    echo >> recon.txt
    echo "[ROBOTS 443] ----------------------------------------------------------" | tee -a recon.txt
    curl -k -O https://"$domain"/robots.txt > /dev/null 2>&1
    cat robots.txt >> recon.txt
    echo >> recon.txt
    rm robots.txt

    ### CHECKING WAF >> recon.txt
    echo "[WAFW00F] -------------------------------------------------------------" | tee -a recon.txt
    wafw00f "$domain" | tail -n +16 | tee -a recon.txt

    ### IDENTIFY TECHNOLOGY >> recon.txt
    echo "[WHATWEB 80] ---------------------------------------------------------" | tee -a recon.txt
    whatweb -a 3 "http://$domain" -H "$cookie" | tee -a recon.txt
    echo "[WHATWEB 443] ---------------------------------------------------------" | tee -a recon.txt
    whatweb -a 3 "https://$domain" -H "$cookie" | tee -a recon.txt

    ### CMS SCAN >> recon.txt
    echo "[CMSEEK] ---------------------------------------------------------" | tee -a recon.txt
    python3 "$HOME"/tools/CMSeeK/cmseek.py -u "$domain" --follow-redirect
    cat "$HOME"/tools/CMSeeK/Result/"$domain"/cms.json | jq . >> recon.txt
    rm -rf "$HOME"/tools/CMSeeK/Result/"$domain"/

    ### DOWNLOAD COMMUNITY-CURATED TEMPLATES & RUN THE AGAINST DOMAIN
    git clone https://github.com/projectdiscovery/nuclei-templates.git
    echo "[NUCLEI 80] ---------------------------------------------------------" | tee -a recon.txt
    nuclei -u http://"$domain" -t nuclei-templates/ -silent | tee -a recon.txt
    echo "[NUCLEI 443] ---------------------------------------------------------" | tee -a recon.txt
    nuclei -u https://"$domain" -t nuclei-templates/ -silent | tee -a recon.txt

    ### VULN SCAN >> recon.txt
    # Perform a brute-force attack to uncover known and potentially dangerous scripts on the web server.
    nl=$'\n'
    cr=$'\r'
    echo "[NIKTO 443] ---------------------------------------------------------" | tee -a recon.txt
    "$HOME"/tools/nikto/program/nikto.pl -host https://"$domain" -useragent "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36${cr}${nl}$cookie" --maxtime 1500 | tee -a recon.txt
    echo "[NIKTO 80] ----------------------------------------------------------" | tee -a recon.txt
    "$HOME"/tools/nikto/program/nikto.pl -host http://"$domain" -useragent "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36${cr}${nl}$cookie" --maxtime 1500 | tee -a recon.txt

    ### TEST HOP-BY-HOP DELETION (only on main page) > recon.txt
    echo -e "\033[0;31m [+]\033[0m TESTING HOP-BY-HOP DELETION"
    echo "[HBH 443] ---------------------------------------------------------" | tee -a recon.txt
    cat "$HOME"/tools/CRIMSON/words/exp/hbh-headers | while IFS= read -r HEADER; do python "$HOME"/tools/hop-by-hop/hbh-header-abuse-test.py -u "https://$domain" -x "$HEADER"; sleep 1; done | tee -a recon.txt

    ### TEST HOP-BY-HOP DELETION (only on main page) >> recon.txt
    echo -e "\033[0;31m [+]\033[0m TESTING HOP-BY-HOP DELETION"
    echo "[HBH 80] ---------------------------------------------------------" | tee -a recon.txt
    cat "$HOME"/tools/CRIMSON/words/exp/hbh-headers | while IFS= read -r HEADER; do python "$HOME"/tools/hop-by-hop/hbh-header-abuse-test.py -u "http://$domain" -x "$HEADER"; sleep 1; done | tee -a recon.txt

    ### SPIDER 1 > urls.txt
    echo -e "\033[0;31m [+]\033[0m STARTING SPIDERS"
    echo -e "\033[0;31m [+]\033[0m SPIDER [1]"
    gospider -q -r -w -a --sitemap --robots -c 10 -s  https://"$domain" -H "$cookie" >> urls.txt

    ### SPIDER 2 >> urls.txt
    echo -e "\033[0;31m [+]\033[0m SPIDER [2]"
    python3 "$HOME"/tools/ParamSpider/paramspider.py -d "$domain" --output ./paramspider.txt --level high > /dev/null 2>&1
    cat paramspider.txt | grep http | sort -u | grep "$domain" >> urls.txt
    cat ../paramspider.txt | grep http | sort -u | grep "$domain" | anew urls.txt > /dev/null

    ### SPIDER 3 >> urls.txt
    echo -e "\033[0;31m [+]\033[0m SPIDER [3]"
    gau "$domain" >> urls.txt

    ### SPIDER 4 >> urls.txt
    echo -e "\033[0;31m [+]\033[0m SPIDER [4]"
    waybackurls "$domain" >> urls.txt

    ### SPIDER 5 >> urls.txt
    echo -e "\033[0;31m [+]\033[0m SPIDER [5]"
    echo http://"$domain" | hakrawler -insecure -subs -u -h "$cookie" >> urls.txt
    echo https://"$domain" | hakrawler -insecure -subs -u -h "$cookie" >> urls.txt
    
    ### SPIDER 6 >> urls.txt
    echo -e "\033[0;31m [+]\033[0m SPIDER [6]"
    galer -u http://"$domain" -s >> urls.txt
    galer -u https://"$domain" -s >> urls.txt

    ### MERGE SPIDERS AND DELETE DUPLICATES >> urls.txt
    echo -e "\033[0;31m [+]\033[0m MERGING SPIDERS RESULTS"
    cat ../urls.txt | grep "$domain" | anew urls.txt > /dev/null
    cat urls.txt | qsreplace -a > temp1.txt
    mv temp1.txt urls.txt

    ### GET NEW ENDPOINTS FROM SPIDERS AND ADD THEM TO WORDLIST FOR DIRECOTRY BRUTEFORCING > custom_dir.txt
    echo -e "\033[0;31m [+]\033[0m GATHERING NEW PATHS FROM SPIDERS RESULTS"
    cat urls.txt | unfurl paths > temp1.txt
    sort -u "$HOME"/tools/CRIMSON/words/dir > "$HOME"/tools/CRIMSON/words/custom_dir.txt
    sort -u temp1.txt | anew "$HOME"/tools/CRIMSON/words/custom_dir.txt > /dev/null
    rm temp1.txt

    ### --- AUTOMATED SECTION --- ### -a
    echo -e "\033[0;31m [+]\033[0m STARTING DIRECTORY BRUTEFORCING"
    if [ $fully_automated == 1 ]
    then
        ### DIRECTORY BRUTEFORCING USING FFUF > status_ffuf.txt
        if [ $https_on == 1 ]
        then
            ffuf -w "$HOME"/tools/CRIMSON/words/custom_dir.txt -t 20 -u https://"$domain"/FUZZ -mc all -fc 400 -H "$cookie" -o ffuf.json > /dev/null
        else
          ffuf -w "$HOME"/tools/CRIMSON/words/custom_dir.txt -t 20 -u http://"$domain"/FUZZ -mc all -fc 400 -H "$cookie" -o ffuf.json > /dev/null
        fi
        cat ffuf.json | jq -c '.results[] | {url:.url,status: .status}' > status_ffuf.txt
        rm ffuf.json

        ### REMOVE TRASH RESPONSES FROM PREVIOUS SCAN > ffuf.txt
        echo -e "\033[0;31m [+]\033[0m REMOVING TRASH RESPONSES FROM status_ffuf.txt"
        python3 "$HOME"/tools/CRIMSON/scripts/clever_ffuf.py
        sort -u temp_ffuf.txt > ffuf.txt
        rm temp_ffuf.txt
    else
        ### DIRECTORY BRUTEFORCING USING FEROXBUSTER (output is called ffuf.txt for comatibility reasons)
        if [ $https_on == 1 ]
        then
            feroxbuster --extract-links --no-recursion --redirects --wordlist "$HOME"/tools/CRIMSON/words/custom_dir.txt -o temp/ferox.txt -u "https://$domain" -H "$cookie" -k
        else
            feroxbuster --extract-links --no-recursion --redirects --wordlist "$HOME"/tools/CRIMSON/words/custom_dir.txt -o temp/ferox.txt -u "http://$domain" -H "$cookie"
        fi

        ### MANUAL CHECK - DELETE TRASH FROM FEROXBUSTER OUTPUT
        echo -e "\033[0;31m [+]\033[0m CHECK FEROXBUSTER OUTPUT AND REMOVE FALSE-POSITIVES MANUALLY"
        cd temp || exit
        echo "Check ferox.txt => \"fg\" in terminal"
        kill -s TSTP "$pid"
        read -p "HIT ENTER TO CONTINUE"
        cd ..
        echo -e "\033[0;31m [+]\033[0m FEROXBUSTER SUCCESSFULLY EDITED - CONTINUING THE SCRIPT EXECUTION"

        ### CONVERT THE OUTPUT TO PROPER FORMAT AND DELETE DUPLICATES
        cat temp/ferox.txt | cut -s -d "h" -f 2-1000 | sed "s/^/h/" | qsreplace -a > ffuf.txt
    fi

    ### GATHER ALL SOURCE CODE FROM ffuf.txt AND STORE IT IN DIRECTORY > all_source_code/
    echo -e "\033[0;31m [+]\033[0m GATHERING SOURCE CODE OF BRUTE-FORCED SITES"
    while IFS= read -r url; do echo -e "\033[0;31m [++]\033[0m GATHERING SOURCE CODE OF: $url" && curl -k -s -D - "$url" -H "$cookie" -A "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36" > all_source_code/$(echo "$url" | sed "s/^https:\/\///" | sed "s/^http:\/\///" | sed "s/\//_/g") ;done < ffuf.txt

    ### GET LINKS TO JS FILES AND PREPARE IT FOR EXPLOIT MODULE > exp/jsfiles.txt
    echo -e "\033[0;31m [+]\033[0m GATHERING .js LINKS"
    cat urls.txt ffuf.txt | getJS --complete --nocolors -H "$cookie" | grep "^http" | grep "$DOMAIN" | sed "s/\?.*//" | anew exp/jsfiles.txt

    ### GET LINKS TO JS FILES FROM urls.txt > exp/jsfiles.txt
    cat urls.txt | grep "\.js$" | grep "^http" | grep "$DOMAIN" | sed "s/\?.*//" | anew exp/jsfiles.txt > /dev/null

    ### CHECK FOR LIVE JS LINKS
    echo -e "\033[0;31m [+]\033[0m CHECKING FOR LIVE .js LINKS"
    httpx -silent -l exp/jsfiles.txt -H "$cookie" >> exp/temp_jsfiles.txt
    mv exp/temp_jsfiles.txt exp/jsfiles.txt

    ### GATHER SOURCE CODE FROM JS LINKS AND STORE IT IN >> all_source_code/
    echo -e "\033[0;31m [+]\033[0m GATHERING SOURCE CODE FROM LIVE .js LINKS"
    while IFS= read -r url; do echo -e "\033[0;31m [++]\033[0m GATHERING SOURCE CODE OF: $url" && curl -k -s "$url" -H "$cookie" -A "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36" | js-beautify > all_source_code/$(echo "$url" | sed "s/^https:\/\///" | sed "s/^http:\/\///" | sed "s/\//_/g") ;done < exp/jsfiles.txt

    ### DIG API KEYS / ENDPOINTS ETC. > zile.txt
    echo -e "\033[0;31m [+]\033[0m GATHERING API KEYS && NEW PATHS"
    cd all_source_code || exit
    python3 "$HOME"/tools/CRIMSON/scripts/zile/zile.py --file >> ../temp/temp_zile.txt
    cd ..
    awk '!seen[$0]++' temp/temp_zile.txt | grep -v "[+]" > zile.txt

    ### GET ENDPOINTS FROM ZILE > extracted.txt
    cat zile.txt | cut -s -d " " -f 2 | sed "s/^..//g" | sed "s/..$//g" | sed "s/\"//g" | unfurl path | sed "s/'$//" > temp/temp_zile_endpoints.txt
    awk '!seen[$0]++' temp/temp_zile_endpoints.txt >> extracted.txt

    ### MUTATE URLS && CHECK IF THERE ARE NO DUPLICATES WITH ffuf.txt > new_endpoints.txt
    while IFS= read -r line; do echo http://"$domain""$line" >> temp/temp_new.txt; done < extracted.txt
    while IFS= read -r line; do echo https://"$domain""$line" >> temp/temp_new.txt; done < extracted.txt
    rm extracted.txt
    awk '!seen[$0]++' temp/temp_new.txt > temp/temp_new_endpoints.txt
    sort ffuf.txt temp/temp_new_endpoints.txt | uniq -d > temp/temp_duplicates.txt
    grep -v -x -f temp/temp_duplicates.txt temp/temp_new_endpoints.txt > new_endpoints.txt

    ### CHECK STATUS OF NEW URLS > status_new_endpoints.txt
    echo -e "\033[0;31m [+]\033[0m CHECKING STATUS CODE OF NEW PATHS"
    wfuzz -f status_new_endpoints.txt,raw -L -Z -z file,new_endpoints.txt -z file,"$HOME"/tools/CRIMSON/words/blank -H "$cookie" FUZZFUZ2Z > /dev/null 2>&1
    rm new_endpoints.txt

    ### REMOVE 400 && 404 RESPONSES
    cat status_new_endpoints.txt | grep "http" | grep -E "C=400  |C=404  " -v | grep -v "Pycurl" | cut -s -d "\"" -f2 > filtered_new_endpoints.txt

    ### MERGE ffuf.txt WITH NEW ENDPOINTS >> ffuf.txt
    echo -e "\033[0;31m [+]\033[0m ADDING LIVE PATHS TO ffuf.txt"
    cat filtered_new_endpoints.txt | anew ffuf.txt > /dev/null
    rm filtered_new_endpoints.txt

    ### ADD http://$domain AND https://$domain EVEN IF THEY ARE 404/400/X status code >> ffuf.txt
    echo -e "http://$domain\nhttps://$domain" | anew ffuf.txt

    ### --- PROXY SECTION --- ### -y
    if [ $proxy_on == 1 ]
    then
        ### PROXY ALL BRUTEFORCED FILES AND DIRECTORIES TO BURP > status_dir.txt
        echo -e "\033[0;31m [+]\033[0m PROXING ALL DIRECTORIES && FILES TO BURP SUITE"
        wfuzz -f status_dir.txt,raw -L -Z -z file,ffuf.txt -z file,"$HOME"/tools/CRIMSON/words/blank -p 127.0.0.1:8080 -H "$cookie" FUZZFUZ2Z > /dev/null 2>&1

        ### CHECK STATUS OF URLS WITH QUERIES && PROXY TO BURP > status_params.txt
        echo -e "\033[0;31m [+]\033[0m PROXING urls.txt TO BURP"
        cat urls.txt | grep "?" > temp/temp_params.txt
        wfuzz -f status_params.txt,raw -L -Z -z file,temp/temp_params.txt -z file,"$HOME"/tools/CRIMSON/words/blank -p 127.0.0.1:8080 -H "$cookie" FUZZFUZ2Z > /dev/null 2>&1
    else
        ### IF THERE IS NO PROXY FLAG - JUST CHECK THE STATUS CODE AND SAVE THE RESULTS FOR FURTHER PROCESSING 
        echo -e "\033[0;31m [+]\033[0m CHECKING STATUS CODE OF (ffuf.txt) BRUTEFORCED DIRECTORIES"
        wfuzz -f status_dir.txt,raw -L -Z -z file,ffuf.txt -z file,"$HOME"/tools/CRIMSON/words/blank -H "$cookie" FUZZFUZ2Z > /dev/null 2>&1
        echo -e "\033[0;31m [+]\033[0m CHECKING STATUS CODE OF (urls.txt) CRAWLED QUERIES"
        cat urls.txt | grep "?" > temp/temp_params.txt
        wfuzz -f status_params.txt,raw -L -Z -z file,temp/temp_params.txt -z file,"$HOME"/tools/CRIMSON/words/blank -H "$cookie" FUZZFUZ2Z > /dev/null 2>&1
    fi

    ### EXTRACT UNIQUE QUERIES > exp/params.txt
    echo -e "\033[0;31m [+]\033[0m PREPARING FILES FOR crimson_exploit MODULE"
    cat status_params.txt | grep -v "C=404 " | grep http | grep -v "Pycurl" | cut -s -d "\"" -f2 | sort -u | qsreplace -a > exp/params.txt

    ### PREAPRE DIRECTORIES FOR EXPLOIT MODULE (filtering static content) > exp/dirs.txt
    cat status_dir.txt | grep -v "C=400\|C=429\|C=404" | grep http | cut -s -d "\"" -f2 | grep -v -e "Pycurl\|\.aac\|\.aiff\|\.ape\|\.au\|\.flac\|\.gsm\|\.it\|\.m3u\|\.m4a\|\.mid\|\.mod\|\.mp3\|\.mpa\|\.pls\|\.ra\|\.s3m\|\.sid\|\.wav\|\.wma\|\.xm\|\.ods\|\.xls\|\.xlsx\|\.csv\|\.ics\|\.vcf\|\.3dm\|\.3ds\|\.max\|\.bmp\|\.dds\|\.gif\|\.jpg\|\.jpeg\|\.png\|\.psd\|\.xcf\|\.tga\|\.thm\|\.tif\|\.tiff\|\.yuv\|\.ai\|\.eps\|\.ps\|\.svg\|\.dwg\|\.dxf\|\.gpx\|\.kml\|\.kmz\|\.webp\|\.3g2\|\.3gp\|\.aaf\|\.asf\|\.avchd\|\.avi\|\.drc\|\.flv\|\.m2v\|\.m4p\|\.m4v\|\.mkv\|\.mng\|\.mov\|\.mp2\|\.mp4\|\.mpe\|\.mpeg\|\.mpg\|\.mpv\|\.mxf\|\.nsv\|\.ogg\|\.ogv\|\.ogm\|\.qt\|\.rm\|\.rmvb\|\.roq\|\.srt\|\.svi\|\.vob\|\.webm\|\.wmv\|\.yuv\|\.7z\|\.a\|\.apk\|\.ar\|\.bz2\|\.cab\|\.cpio\|\.deb\|\.dmg\|\.egg\|\.gz\|\.iso\|\.jar\|\.lha\|\.mar\|\.pea\|\.rar\|\.rpm\|\.s7z\|\.shar\|\.tar\|\.tbz2\|\.tgz\|\.tlz\|\.war\|\.whl\|\.xpi\|\.zip\|\.zipx\|\.xz\|\.pak\|\.js\|\.jsmin\|\.css\|\.js\|\.jsx\|\.less\|\.scss\|\.wasm\|\.eot\|\.otf\|\.ttf\|\.woff\|\.woff2\|\.ppt\|\.odp\|\.doc\|\.docx\|\.ebook\|\.log\|\.md\|\.msg\|\.odt\|\.org\|\.pages\|\.pdf\|\.rtf\|\.rst\|\.tex\|\.txt\|\.wpd\|\.wps\|\.mobi\|\.epub\|\.azw1\|\.azw3\|\.azw4\|\.azw6\|\.azw\|\.cbr\|\.cbz" | sort -u > exp/dirs.txt

    ### PREPARE FILES WORDLIST FOR BACKUPER AND ARJUN > exp/files.txt
    cat status_dir.txt | grep -v "C=400\|C=429\|C=404" | grep http | cut -s -d "\"" -f2 | grep -v -e "Pycurl\|\.woff\|\.svg\|\.png\|\.gif\|\.jpg\|\.png\|\.css\|\.mp3\|\.mp4" | sed "s/\/$//g" | sort -u | grep -v "^https://$domain$\|^http://$domain$" > files.txt

    ### CHECK FOR BACKUP FILES > backups.txt
    echo -e "\033[0;31m [+]\033[0m CHECKING EXISTANCE OF BRUTEFORCED FILES BACKUPS"
    python "$HOME"/tools/CRIMSON/scripts/crimson_backuper.py -w files.txt -e "$HOME"/tools/CRIMSON/words/BCK_EXT -c "$cookie" -o backups.txt

    ### CORS MISCONFIGURATION SCAN >> recon.txt
    echo "[CORS] ---------------------------------------------------------" | tee -a recon.txt
    cat ffuf.txt | CorsMe -t 50 -header "$cookie" -output corsme.txt > /dev/null 2>&1
    cat corsme.txt >> recon.txt
    rm corsme.txt
    rm error_requests.txt

    ### DIG SECRETS FROM all_source_code/ SAVE COLORED OUTPUT > apikeys.txt
    echo -e "\033[0;31m [+]\033[0m CHECKING SECRETS IN GATHERED SOURCE CODE"
    grep -EHirn "accesskey|admin|aes|api_key|apikey|checkClientTrusted|crypt|password|pinning|secret|SHA256|SharedPreferences|superuser|token|X509TrustManager|google_api|google_api|google_captcha|google_oauth|amazon_aws_access_key_id|amazon_mws_auth_toke|amazon_aws_url|facebook_access_token|authorization_basic|authorization_bearer|authorization_api|mailgun_api_key|twilio_api_key|twilio_account_sid|twilio_app_sid|paypal_braintree_access_token|square_oauth_secret|square_access_token|stripe_standard_api|stripe_restricted_api|github_access_tokenttp" all_source_code/ --color=always > apikeys.txt
    detect-secrets scan all_source_code/ --all-files > detect-secrets.txt

    ### GET PARAMETER NAMES BY BRUTEFORCING THEM > exp/arjun.txt
    echo -e "\033[0;31m [+]\033[0m STARTING ARJUN ON exp/files.txt - IT WILL TAKE A WHILE..."
    arjun -i files.txt -oT arjun.txt -q --headers "$cookie"
    
    ### CLEAR LOGS
    rm files.txt
    rm status_ffuf.txt
    rm status_new_endpoints.txt
    ### REMOVE EMPTY FILES AND DIRECTORIES
    find . -type d -empty -print -delete -o -type f -empty -print -delete
    
    ### MARK THE END
    echo "----------------------------------------------------------------"
    echo "SCANNING END: $(date +'[%m-%d %H:%M:%S]')" >> recon.txt    
    echo "----------------------------------------------------------------"

    echo -e "\033[0;31m [+]\033[0m 1. recon.txt :"
    cat recon.txt
    echo -e "\033[0;31m [+]\033[0m 2. zile.txt :"
    cat zile.txt | cut -s -d " " -f 2 | sed "s/^..//g" | sed "s/..$//g" | sort -u | sed "s/^.//" | sed "s/.$//"
    echo -e "\033[0;31m [+]\033[0m 3. CHECK :"
    echo "  - status_dir.txt"
    echo "  - status_params.txt"
    echo "  - arjun.txt"
    echo "  - backups.txt"
    echo "  - apikeys.txt"
    echo "  - detect-secrets.txt"
    echo "  - use pywhat && ciphey for 'strange' findings"
    echo "  - robots.txt and sitemap.xml  at : https://httpstatus.io/"
fi